{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# This Cell generates the training pictures out of the tif files if needed\n# For the mini-image generation it is needed to split the images into smaller ones, beacause ram is limited\n\nfrom operator import eq\nfrom re import I\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport PIL\nfrom ctypes import cdll\nfrom PIL import Image\nfrom openslide import OpenSlide\nPIL.Image.MAX_IMAGE_PIXELS = 2331200000\ntrainingPath: str = '../input/mayo-clinic-strip-ai/train/'\ntestPath: str = '../input/mayo-clinic-strip-ai/test/'\ngeneratedTrainingDataPath = '../training-data/'  # label/imgId/sliceNumber\ntrainingDataPathList = ['../training-data/', '../training-data2/', '../training-data3/', '../training-data4/'] # training data will be generated 3 times to get different data for the 3 different models\n\nreGenerateTrainingDataLAA = False\nreGenerateTrainingDataCE = False\ntargetQuantityOfTilesPerFieldLAA = 2200\ntargetQuantityOfTilesPerFieldCE = 600\n\ntrainingMode = False\n\n# load train.csv into pandas dataframe\nif trainingMode:\n    trainDataMetaTable = pd.read_csv('../input/mayo-clinic-strip-ai/train.csv')\n\n\ndef checkInnerTileVariance(imgNDArray, startX, startY, height, width, numberOfSamples, avgDifferenceNeeded, maxAcceptedEquals, equalsDecrementIfUnequal, equalsIncrementIfEqual, maxAbsoluteDifferenceToBeEquals):\n    # get randome pixels in changing quaters in the tileArea and check the difference between the rgb values\n    # if the average difference is to small, the image area is not useful (white background for example would not be usefull)\n    lastPixelRGBValues = imgNDArray[startX, startY]\n    differenceSum = 0\n    equalsCount = 0\n    quaterChangeInterval = int(numberOfSamples / 4)\n    numberOfSamplesDoneCurrentQuater = 0\n    halfWidth = int(width / 2)\n    halfHeight = int(height / 2)\n    startXOffset = 0\n    startYOffset = 0\n    for i in range(numberOfSamples):\n        startXWithOffset = startX + startXOffset\n        startYWithOffset = startY + startYOffset\n        randomX = np.random.randint(startXWithOffset, startXWithOffset + halfWidth)\n        randomY = np.random.randint(startYWithOffset, startYWithOffset + halfHeight)\n        currentPixelRGBValues = imgNDArray[randomX, randomY]\n        differenceAbsolute = abs(int(currentPixelRGBValues[0]) - int(lastPixelRGBValues[0])) + abs(int(\n            currentPixelRGBValues[1]) - int(lastPixelRGBValues[1])) + abs(int(currentPixelRGBValues[2]) - int(lastPixelRGBValues[2]))\n        # if the pixels are (almost) the same\n        if differenceAbsolute <= maxAbsoluteDifferenceToBeEquals:\n            equalsCount += equalsIncrementIfEqual\n            if equalsCount > maxAcceptedEquals:\n                return False\n        else:\n            equalsCount -= equalsDecrementIfUnequal\n        differenceSum += differenceAbsolute\n        lastPixelRGBValues = currentPixelRGBValues\n        numberOfSamplesDoneCurrentQuater += 1\n        if numberOfSamplesDoneCurrentQuater >= quaterChangeInterval:\n            numberOfSamplesDoneCurrentQuater = 0\n            if startXOffset == 0:\n                startXOffset = halfWidth\n            else:\n                startXOffset = 0\n                if startYOffset == 0:\n                    startYOffset = halfHeight\n                else:\n                    startYOffset = 0\n    return differenceSum / numberOfSamples >= avgDifferenceNeeded\n\n\ndef createRandomTilesFromImage(deletePastResults, fieldNumber, imgResultPath, imgNDArray, imgName, numberOfTilesToGenerate, tileWidth, tileHeight,saveTile):\n    numberOfTilesGenerated = 0\n    if saveTile:\n        if deletePastResults:\n            # remove all files in the folder\n            if os.path.exists(imgResultPath):\n                for file in os.listdir(imgResultPath):\n                    os.remove(os.path.join(imgResultPath, file))\n        if not os.path.exists(imgResultPath):\n            os.mkdir(imgResultPath)\n    maxNumberOfTriesPerTile = 50\n    # if more than 75% of the current 1/9 of the 3x3 grid is useless, than it should not get that much tiles\n    maxNumberOfTriesAbsolute = numberOfTilesToGenerate * 4\n    numberOfTriesDoneCurrentTile = 0\n    numberOfTriesDoneAbsolute = 0\n    # the area for the final image is half the tile size (the tile gets reduced and loses some detail information) to get a better overview for the model withouth loosing to much performance while training\n    generatedTileWidth = int(tileWidth / 2)\n    generatedTileHeight = int(tileHeight / 2)\n    generatedTileList = []\n    while numberOfTilesGenerated < numberOfTilesToGenerate and numberOfTriesDoneCurrentTile < maxNumberOfTriesPerTile and numberOfTriesDoneAbsolute < maxNumberOfTriesAbsolute:\n        # get random coordinates\n        startX = np.random.randint(0, imgNDArray.shape[0] - tileWidth)\n        startY = np.random.randint(0, imgNDArray.shape[1] - tileHeight)\n        numberOfTriesDoneCurrentTile += 1\n        numberOfTriesDoneAbsolute += 1\n        # check if the tile is usefull\n        if checkInnerTileVariance(imgNDArray=imgNDArray, startX=startX, startY=startY, height=tileHeight, width=tileWidth, numberOfSamples=200, avgDifferenceNeeded=128, maxAcceptedEquals=20, equalsDecrementIfUnequal=1, equalsIncrementIfEqual=2, maxAbsoluteDifferenceToBeEquals=8):\n            # write every second pixel to a new image\n            generatedTile: Image = Image.new(\n                'RGB', (generatedTileWidth, generatedTileHeight))\n            for x in range(generatedTileWidth):\n                for y in range(generatedTileHeight):\n                    # put pixel withouth alpha channel\n                    currentPixel = imgNDArray[startX + x * 2, startY + y * 2]\n                    generatedTile.putpixel(\n                        (x, y), (currentPixel[0], currentPixel[1], currentPixel[2]))\n            # save the image\n            if saveTile:\n                generatedTile.save(\n                    imgResultPath + 'fieldNumber' + str(fieldNumber)+'_' + 'tileNumber' + str(numberOfTilesGenerated) + '.tif') #used for training\n            else:\n                generatedTileList.append(np.array(generatedTile)) #used for testing\n            numberOfTilesGenerated += 1\n            numberOfTriesDoneCurrentTile = 0\n    return generatedTileList\n\n\ndef imageSplittingAndTileGeneration(imageId, imageFolder, imgLabel, saveTile, optionalNumberOfTilesToGeneratePerField = 0 , tileListToFill = []):\n    # fields are parts of the 3x3 grid the images get split into (this is done due to memory limitations)\n    # tiles are randome but usefull parts of the fields (the tiles are the training data or the test data)\n    # open the image\n    slide = OpenSlide(imageFolder + imageId + '.tif')\n    # get the image size\n    width, height = slide.dimensions\n    # slice the images into 3x3 tiles\n    if height < 200:\n        return tileListToFill\n    if width < 200:\n        return tileListToFill\n    fieldCount = 0\n    numberOfRows = (int)(height / 16000) + 1\n    numberOfColumns = (int)(width / 16000) + 1\n    numberOfFields = numberOfRows * numberOfColumns\n    # create numberOftilesToGenerate depending on targetQuantityOfTilesPerFieldLAA and targetQuantityOfTilesPerFieldCE depends on label\n    numberOfTilesToGeneratePerField = 0\n    if imgLabel == 'LAA':\n        numberOfTilesToGeneratePerField = targetQuantityOfTilesPerFieldLAA\n    else:\n        numberOfTilesToGeneratePerField = targetQuantityOfTilesPerFieldCE\n    if optionalNumberOfTilesToGeneratePerField > 0:\n        numberOfTilesToGeneratePerField = optionalNumberOfTilesToGeneratePerField\n    # get the tile size\n    fieldWidth = int(width / numberOfColumns)\n    fieldHeight = int(height / numberOfRows)\n    for i in range(0, numberOfRows):\n        for j in range(0, numberOfColumns):\n            imgName = imageId\n            # get the tile\n            field = slide.read_region(\n                (i * fieldWidth, j * fieldHeight), 0, (fieldWidth, fieldHeight))\n            # convert the tile to a numpy array\n            field = np.array(field)\n            if imgLabel is not None:\n                # try generate 300 tiles from the field\n                tiles = createRandomTilesFromImage(deletePastResults = (fieldCount == 0), fieldNumber=fieldCount, imgResultPath=generatedTrainingDataPath +\n                                           imgLabel + '/' + imgName + '/', imgNDArray=field, imgName=imgName, numberOfTilesToGenerate=numberOfTilesToGeneratePerField, tileWidth=160, tileHeight=160, saveTile=saveTile)\n                tileListToFill.extend(tiles)\n            fieldCount += 1\n            print('fieldCount: ' + str(fieldCount) + ' of ' + str(numberOfFields) + ' for image ' + imageId + ' done')\n    return tileListToFill\n\n\ndef generateTrainingData():\n    # iterate through all tif files in '../input/train/' and create 300 tiles for each image\n    # iterate through the rows of the dataframe\n    for index, row in trainDataMetaTable.iterrows():\n        # get the image_id\n        imageId = row['image_id']\n        # get the image_label\n        imageLabel = row['label']\n        if (imageLabel == 'CE' and not reGenerateTrainingDataCE) or (imageLabel == 'LAA' and not reGenerateTrainingDataLAA):\n            continue\n        # check if the image exists\n        if os.path.exists(trainingPath + imageId + '.tif'):\n            imageSplittingAndTileGeneration(imageId, trainingPath, imageLabel, True)\n        else:\n            print('image ' + imageId + ' does not exist')\n        # print the image number done\n        print('image ' + str(index) + ' done')\n\n# check if the training data is already generated\nif trainingMode:\n    for trainingDataPath in trainingDataPathList:\n        generatedTestDataPath = trainingDataPath\n        numberOfExistingTrainingImageFoldersSufficentCE = False\n        numberOfExistingTrainingImageFoldersSufficentLAA = False\n        if os.path.exists(generatedTrainingDataPath):\n            numberOfExistingTrainingImageFoldersCE = len(\n                os.listdir(generatedTrainingDataPath + '/CE/')) \n            numberOfExistingTrainingImageFoldersLAA =  len(os.listdir(generatedTrainingDataPath + '/LAA/'))\n            numberOfExistingTrainingImageFoldersSufficentLAA = int(\n                numberOfExistingTrainingImageFoldersLAA * 1.1) >= len(trainDataMetaTable[trainDataMetaTable['label'] == 'LAA'])\n            numberOfExistingTrainingImageFoldersSufficentCE = int(\n                numberOfExistingTrainingImageFoldersCE * 1.1) >= len(trainDataMetaTable[trainDataMetaTable['label'] == 'CE'])\n        else:\n            os.mkdir(generatedTrainingDataPath)\n            os.mkdir(generatedTrainingDataPath + '/CE/')\n            os.mkdir(generatedTrainingDataPath + '/LAA/')\n        if not numberOfExistingTrainingImageFoldersSufficentLAA:\n            reGenerateTrainingDataLAA = True\n        if not numberOfExistingTrainingImageFoldersSufficentCE:\n            reGenerateTrainingDataCE = True\n        if reGenerateTrainingDataLAA or reGenerateTrainingDataCE:\n            generateTrainingData()\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"execution":{"iopub.status.busy":"2022-10-05T13:27:37.291976Z","iopub.execute_input":"2022-10-05T13:27:37.292520Z","iopub.status.idle":"2022-10-05T13:27:37.502556Z","shell.execute_reply.started":"2022-10-05T13:27:37.292403Z","shell.execute_reply":"2022-10-05T13:27:37.501251Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#this cell creates, loads or trains the basic models wich are used for Monte-Carlo-Predicitions on multiple different randome samples of the image\n# import keras\nimport random\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\n\nbasicModelPath = '../input/basicmodels4/'\nlogLossEvaluationModelPath = '../input/losslogevaluationmodel4/'\nuseTrainingDataPath =  '../training-data/'\ntrainingDataFoldersLAA = []\ntrainingDataFoldersCE = []\ntrainingDataFoldersContent = {}\nnumberOfTrainingSamplesPerIteration = 2048\nnumberOfInputImagesForOneModel = 4\nnumberOfClasses = 2\nCE_INDEX = 0\nLAA_INDEX = 1\nif trainingMode:\n    accumulatedData = np.zeros(\n    (numberOfTrainingSamplesPerIteration, numberOfInputImagesForOneModel, 80, 80, 3), dtype=float)\n    accumulatedLabels = np.zeros((numberOfTrainingSamplesPerIteration,numberOfClasses), dtype=float)\n\ndataLoadingFolderSuccessMinimumFactorModelList = [0.62, 0.2, 0.4]\ndataLoadingFolderSuccessMinimumFactor = 0.60 # if more than 40% of training data folders/images are failing, cause of repeating data, the training is stopped, this protects against overfitting while making efficient use of randomly loaded training data\ndataLoadingFolderSuccessInflationFactor = 0.9901\nnewDataLoadingSuccess = 1.0\n\n#dictionary\npictureUsageCheckDictionary = {} #this dictionary gets filled with picture names as keys and boolean values as values, where True means the picture got already used for the input-picture-number corresponding to the index of the models input pictures.\n\ndef createModelType1():\n    # create the model\n    inputLayer = keras.Input(shape=(numberOfInputImagesForOneModel, 80, 80, 3))\n    conv1 = Conv2D(32, kernel_size=(3, 3), strides=(2, 2), activation='relu')(inputLayer)\n    conv2 = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), activation='relu')(conv1)\n    conv3 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu')(conv2)\n    conv4 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu')(conv3)\n    flatten = Flatten()(conv4)\n    dense1 = Dense(4096, activation='relu')(flatten)\n    dense2 = Dense(2048, activation='relu')(dense1)\n    dense3 = Dense(1024, activation='relu')(dense2)\n    dense4 = Dense(512, activation='relu')(dense3)\n    dense5 = Dense(128, activation='relu')(dense4)\n    outputLayer = Dense(2, activation='sigmoid')(dense5)\n    model = keras.Model(inputs=inputLayer, outputs=outputLayer)\n    # compile the model\n    model.compile(\n        optimizer=Adam(lr=0.00005),\n        loss='mse',\n        metrics=['accuracy','categorical_accuracy']\n    )\n    return model # this model is designed to be trained on 4 images at once, the advantage lies in the fact that the model can learn to recognize patterns in the images, which are not present in the single images, but are present in the combination of the images\n\ndef createModelType2(): #model2 trys compensate the mistakes of model1a and model1b\n    # create convolutional model with functional api\n    inputLayer = keras.Input(shape=(numberOfInputImagesForOneModel, 80, 80, 3))\n    conv1 = Conv2D(32, kernel_size=(3, 3), strides=(2, 2), activation='relu')(inputLayer)\n    conv2 = Conv2D(128, kernel_size=(4, 4), strides=(2, 2), activation='relu')(conv1)\n    conv3 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu')(conv2)\n    conv4 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), activation='relu')(conv3)\n    flatten = Flatten()(conv4)\n    #add concat additional feature with 512 inputs to flatten\n    inputLayer2 = keras.Input(shape=(4)) #the last layer of model1a.h5 and model1b.h5 are concatenated to this layer\n    concat = keras.layers.concatenate([flatten, inputLayer2])\n    dense1 = Dense(5000, activation='relu')(concat) \n    dense2 = Dense(2500, activation='relu')(dense1)\n    dense3 = Dense(1250, activation='relu')(dense2)\n    dense4 = Dense(1250, activation='relu')(dense3)\n    dense5 = Dense(512, activation='relu')(dense4)\n    dense6 = Dense(128, activation='relu')(dense5)\n    outputLayer = Dense(2, activation='sigmoid')(dense6)\n    model = keras.Model(inputs=[inputLayer, inputLayer2], outputs=outputLayer)\n    # compile the model\n    model.compile(\n        optimizer=Adam(lr=0.00005),\n        loss='mse',\n        metrics=['accuracy','categorical_accuracy']\n    )\n    return model # this model is designed to be trained on 4 images at once, the advantage lies in the fact that the model can learn to recognize patterns in the images, which are not present in the single images, but are present in the combination of the images\n    \n\ndef loadTrainingDataFolders(trainingDataPath):\n    global trainingDataFoldersLAA\n    global trainingDataFoldersCE\n    global trainingDataFoldersContent\n    trainingDataFoldersLAA.clear()\n    trainingDataFoldersCE.clear()\n\n    for folder in os.listdir(trainingDataPath + '/CE/'):\n        if len(os.listdir(trainingDataPath + '/CE/' + folder)) < numberOfInputImagesForOneModel:\n            continue\n        trainingDataFoldersCE.append(trainingDataPath + 'CE/' + folder + '/')\n    for folder in os.listdir(trainingDataPath + '/LAA/'):\n        # check if the folder is empty\n        if len(os.listdir(trainingDataPath + '/LAA/' + folder)) < numberOfInputImagesForOneModel:\n            continue\n        trainingDataFoldersLAA.append(trainingDataPath + 'LAA/' + folder + '/')\n\n    # load the content paths of the training data folders\n    for folder in trainingDataFoldersLAA:\n        trainingDataFoldersContent[folder] = []\n        for file in os.listdir(folder):\n            trainingDataFoldersContent[folder].append(folder + file)\n    for folder in trainingDataFoldersCE:\n        trainingDataFoldersContent[folder] = []\n        for file in os.listdir(folder):\n            trainingDataFoldersContent[folder].append(folder + file)\n    \n\ndef loadTrainingData():\n    global accumulatedData\n    global accumulatedLabels\n    global pictureUsageCheckDictionary\n    global newDataLoadingSuccess\n    nextIsCE = True\n    maxNumberOfInFolderFails = 3\n    newDataLoadingSuccess = 1.0\n    dataLoadingFolderAdditionOnSuccess = 0.01\n    # load the data\n    # iterate through the rows of the dataframe\n    i = 0\n    while i < numberOfTrainingSamplesPerIteration:\n        # choose random image folder\n        if nextIsCE:\n            folder = random.choice(trainingDataFoldersCE)\n        else:\n            folder = random.choice(trainingDataFoldersLAA)\n        numberOfFailsCounter = 0\n        inputImageNumber = 0\n        folderFailed = False\n        #get 4 random images from the folder\n        while inputImageNumber < numberOfInputImagesForOneModel:\n            #choose random image\n            imagePath = random.choice(trainingDataFoldersContent[folder])\n            usageList = pictureUsageCheckDictionary.get(imagePath)\n            if usageList == None:\n                usageList = [False] * numberOfInputImagesForOneModel\n                pictureUsageCheckDictionary[imagePath] = usageList\n            if not usageList[inputImageNumber]:\n                usageList[inputImageNumber] = True\n                # use PIL to load the image\n                img = Image.open(imagePath)\n                # load the image into a numpy array\n                img = np.array(img, dtype=float)\n                #normalize the image\n                img *= 1.0 / 255.0\n                # add the image to the accumulated data\n                accumulatedData[i][inputImageNumber] = img\n                inputImageNumber += 1\n            else:\n                numberOfFailsCounter += 1\n                if numberOfFailsCounter > maxNumberOfInFolderFails:\n                    folderFailed = True\n                    break\n        if not folderFailed:\n            # add the label to the labels\n            if nextIsCE:\n                accumulatedLabels[i][CE_INDEX] = 1.0\n                accumulatedLabels[i][LAA_INDEX] = 0.0\n            else:\n                accumulatedLabels[i][LAA_INDEX] = 1.0\n                accumulatedLabels[i][CE_INDEX] = 0.0\n            nextIsCE = not nextIsCE\n            i += 1\n            newDataLoadingSuccess += dataLoadingFolderAdditionOnSuccess\n        newDataLoadingSuccess *= dataLoadingFolderSuccessInflationFactor\n        if newDataLoadingSuccess < dataLoadingFolderSuccessMinimumFactor:\n            print(\"Not enough new training data, stopping training\")\n            break\n    return newDataLoadingSuccess >= dataLoadingFolderSuccessMinimumFactor\n\ndef trainModelType1():\n    # create the model\n    model = createModelType1()\n    # load the training data folders\n    loadTrainingDataFolders(useTrainingDataPath)\n    # train the model\n    iterationCount = 0\n    while loadTrainingData():\n        model.fit(accumulatedData, accumulatedLabels, epochs=1, batch_size=16)\n        iterationCount += 1\n        # print count and successweight\n        print(\"Iteration: \" + str(iterationCount) + \" new data loading success: \" + str(newDataLoadingSuccess))\n    return model\n\ndef trainModelType2(model1a, model1b):\n    # create the model\n    model = createModelType2()\n    # load the training data folders\n    loadTrainingDataFolders(useTrainingDataPath)\n    # train the model\n    iterationCount = 0\n    while loadTrainingData():\n        # get the outputs of the models\n        model1aOutput = model1a.predict(accumulatedData)\n        model1bOutput = model1b.predict(accumulatedData)\n        # concatenate the outputs\n        model2Input = np.concatenate((model1aOutput, model1bOutput), axis=1)\n        # train the model\n        model.fit([accumulatedData, model2Input], accumulatedLabels, epochs=1, batch_size=16)\n        iterationCount += 1\n        # print count and successweight\n        print(\"Iteration: \" + str(iterationCount) + \" new data loading success: \" + str(newDataLoadingSuccess))\n    return model\n\ndef loadModel(modelPath):\n    # load the model\n    model = keras.models.load_model(modelPath)\n    return model\n\n\n# check if model1a.h5 exists, load it else train it\nif os.path.isfile(basicModelPath + 'model1a.h5'):\n    model1a = loadModel(basicModelPath + 'model1a.h5')\nelif trainingMode:\n    useTrainingDataPath = trainingDataPathList[0]\n    dataLoadingFolderSuccessMinimumFactor = dataLoadingFolderSuccessMinimumFactorModelList[0]\n    model1a = trainModelType1()\n    model1a.save('model1a.h5')\nelse:\n    print(\"No model1a.h5 found, stopping\")\n    exit()\n# check if model1b.h5 exists, load it else train it\nif os.path.isfile(basicModelPath + 'model1b.h5'):\n    model1b = loadModel(basicModelPath + 'model1b.h5')\nelif trainingMode:\n    useTrainingDataPath = trainingDataPathList[1]\n    dataLoadingFolderSuccessMinimumFactor = dataLoadingFolderSuccessMinimumFactorModelList[1]\n    model1b = trainModelType1()\n    model1b.save('model1b.h5')\nelse:\n    print(\"No model1b.h5 found, stopping\")\n    exit()\n# check if model2.h5 exists, load it else train it\nif os.path.isfile(basicModelPath + 'model2.h5'):\n    model2 = loadModel(basicModelPath + 'model2.h5')\nelif trainingMode:\n    useTrainingDataPath = trainingDataPathList[2]\n    dataLoadingFolderSuccessMinimumFactor = dataLoadingFolderSuccessMinimumFactorModelList[2]\n    model2 = trainModelType2(model1a, model1b)\n    model2.save('model2.h5')\nelse:\n    print(\"No model2.h5 found, stopping\")\n    exit()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-05T13:27:37.504853Z","iopub.execute_input":"2022-10-05T13:27:37.505245Z","iopub.status.idle":"2022-10-05T13:27:55.910138Z","shell.execute_reply.started":"2022-10-05T13:27:37.505211Z","shell.execute_reply":"2022-10-05T13:27:55.908553Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2022-10-05 13:27:45.475245: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"}]},{"cell_type":"code","source":"#this cell executes the montecarlo samples and trains the final losslogevaluationmodel, which gets the the results of the montecarlo-predictions as input\nfrom threading import Thread\nfrom time import sleep\n\nclass ImageProcessingThread(Thread):\n    def __init__(self,imageId, evaluationFolder, numberOfSamplesPerImageField):\n        Thread.__init__(self)\n        self.imageId = imageId\n        self.evaluationFolder = evaluationFolder\n        self.numberOfSamplesPerImageField = numberOfSamplesPerImageField\n        self.tileList = []\n\n    def run(self):\n        self.tileList = imageSplittingAndTileGeneration(self.imageId, self.evaluationFolder , 'unknown', False, self.numberOfSamplesPerImageField)\n\n#create list of evaluations\nevaluations = {} # key: image_id, value: output of [model1a, model1b, model2] times number of evaluations per image\n#create list of random image tiles\nrandomImageTiles = []\nnumberOfSamplesPerImageField = 250\nevaluationsPerImage = 48\n\nif trainingMode:\n    evaluationFolder = trainingDataPathList[3]\n    dataMetaTable = pd.read_csv('../input/mayo-clinic-strip-ai/train.csv')\n    numberOfTrainingSamples = 128000\n    accumulatedData = np.zeros(\n    (numberOfTrainingSamples, evaluationsPerImage * 6), dtype=float)\n    accumulatedLabels = np.zeros((numberOfTrainingSamples,numberOfClasses), dtype=float)\nelse:\n    evaluationFolder = '../input/mayo-clinic-strip-ai/test/'\n    dataMetaTable = pd.read_csv('../input/mayo-clinic-strip-ai/test.csv')\n    #evaluationFolder = '../input/mayo-clinic-strip-ai/train/'\n    #dataMetaTable = pd.read_csv('../input/testfile2/test2.csv')\n\ndef processImagesForEvaluation(): #each picture gets evaluated on the fly, results get saved in \"evaluations\", later the evaluations get combined in a final log-loss/cross-entropy model to a final result\n    # iterate through the rows of the dataframe\n    # ToDo refactor redundant code\n    thread1 = None\n    thread2 = None\n    thread3 = None\n    # added some multithreading experimantally to increase speed from 15 hours to under 9 hours on kaggle server\n    for index, row in dataMetaTable.iterrows():\n        # get the image id\n        imageId = row['image_id']\n        if os.path.exists(evaluationFolder + imageId + '.tif'):\n            #get file size in byte\n            fileSize = os.path.getsize(evaluationFolder + imageId + '.tif')\n            currentImageGotThread = False\n            while not currentImageGotThread:\n                if thread1 == None:\n                    thread1 = ImageProcessingThread(imageId, evaluationFolder, numberOfSamplesPerImageField)\n                    currentImageGotThread = True\n                    thread1.start()\n                    continue #go to next image\n                if thread2 == None:\n                    thread2 = ImageProcessingThread(imageId, evaluationFolder, numberOfSamplesPerImageField)\n                    currentImageGotThread = True\n                    thread2.start()\n                    continue #go to next image\n                if thread3 == None:\n                    thread3 = ImageProcessingThread(imageId, evaluationFolder, numberOfSamplesPerImageField)\n                    currentImageGotThread = True\n                    thread3.start()\n                    continue\n                # wait for one thread to finish\n                while True:\n                    sleep(0.1)\n                    if thread1 == None or (not thread1.isAlive()):\n                        break\n                    if thread2 == None or (not thread2.isAlive()):\n                        break\n                    if thread3 == None or (not thread3.isAlive()):\n                        break\n                if thread1 != None and (not thread1.isAlive()):\n                    predictAndWriteResult(thread1)\n                    thread1 = None\n                    continue\n                if thread2 != None and (not thread2.isAlive()):\n                    predictAndWriteResult(thread2)\n                    thread2 = None\n                    continue\n                if thread3 != None and (not thread3.isAlive()):\n                    predictAndWriteResult(thread3)\n                    thread3 = None\n                    continue\n        else:\n            print('image ' + imageId + ' does not exist')\n    # wait for the last thread to finish\n    while (thread1 != None and thread1.isAlive()) or (thread2 != None and thread2.isAlive() or (thread3 != None and thread3.isAlive())):\n        sleep(0.1)\n    if thread1 != None:\n        predictAndWriteResult(thread1)\n        thread1 = None\n    if thread2 != None:\n        predictAndWriteResult(thread2)\n        thread2 = None\n    if thread3 != None:\n        predictAndWriteResult(thread3)\n        thread3 = None\n\ndef predictAndWriteResult(threadObject):\n    tileList = threadObject.tileList\n    imageId = threadObject.imageId\n    if len(tileList) > 0:\n        tilesToEvaluate = readRandomSamplesFromTileListForModel(tileList, evaluationsPerImage)\n        evaluations[imageId] = predictTilesWithModels(tilesToEvaluate)\n    print('image ' + imageId + ' done!')\n\ndef readRandomSamplesFromTileListForModel(tileList, numberOfSamples):\n    global randomImageTiles\n    randomImageTiles.clear()\n    for i in range(numberOfSamples):\n        for j in range(numberOfInputImagesForOneModel):\n            randomImageTiles.append(tileList[random.randint(0, len(tileList) - 1)])\n    return randomImageTiles #returns a charge of image tiles to evaluate the image\n\ndef predictTilesWithModels(listOfNDImagesToEvaluate):\n    global evaluations\n    #convert the list of numpy arrays to numpy array\n    listOfNDImagesToEvaluate = np.array(listOfNDImagesToEvaluate, dtype=float)\n    #get the ndarray with shape (numberOfSamples, 80, 80, 3) into the shape (numberOfSamples/numberOfInputImagesForOneModel,numberOfInputImagesForOneModel, 80, 80, 3)\n    listOfNDImagesToEvaluate = listOfNDImagesToEvaluate.reshape((int(len(listOfNDImagesToEvaluate)/numberOfInputImagesForOneModel), numberOfInputImagesForOneModel, 80, 80, 3))\n    #normalize the data 255\n    listOfNDImagesToEvaluate *= 1.0 / 255.0\n    # predict the tiles with the models\n    model1aOutput = model1a.predict(listOfNDImagesToEvaluate)\n    model1bOutput = model1b.predict(listOfNDImagesToEvaluate)\n    model2Input = np.concatenate((model1aOutput, model1bOutput), axis=1)\n    model2Output = model2.predict([listOfNDImagesToEvaluate, model2Input]).flatten()\n    model1aOutput = model1aOutput.flatten()\n    model1bOutput = model1bOutput.flatten()\n    #make outputs one dimensional\n    return np.concatenate((model1aOutput, model1bOutput, model2Output), axis=0)\n\ndef loadTrainingData(): #creats and loads the montecarlo training data into accumulatedData and accumulatedLabels\n    global evaluationFolder\n    global accumulatedData\n    global accumulatedLabels\n    global numberOfTrainingSamples\n    loadTrainingDataFolders(evaluationFolder)\n    # load the data\n    # iterate through the rows of the dataframe\n    i = 0\n    for i in range(numberOfTrainingSamples):\n        # choose random image folder\n        nextIsCE = random.randint(0, len(trainingDataFoldersCE) + len(trainingDataFoldersLAA)) < len(trainingDataFoldersCE)\n        if nextIsCE:\n            folder = random.choice(trainingDataFoldersCE)\n        else:\n            folder = random.choice(trainingDataFoldersLAA)\n        tileList = [] #the list will contain NDArrays of the tiles\n        for k in range(evaluationsPerImage):\n            #choose random image\n            for j in range(numberOfInputImagesForOneModel):\n                imagePath = random.choice(trainingDataFoldersContent[folder])\n                #load the image into the tileList using PIL\n                image = Image.open(imagePath)\n                image = np.array(image)\n                tileList.append(image)\n        predictions = predictTilesWithModels(tileList)\n        accumulatedData[i] = predictions\n        if nextIsCE:\n            accumulatedLabels[i] = [1.0, 0.0]\n        else:\n            accumulatedLabels[i] = [0.0, 1.0]\n        if i % 4000 == 3999:\n            #save\n            np.save('accumulatedData.npy', accumulatedData)\n            np.save('accumulatedLabels.npy', accumulatedLabels)\n            print('loaded ' + str(i + 1) + ' training samples')\n            \ndef createFinalEvaluationLossLogModel(): #create a model that combines the evaluations of the models\n    # create the model using functional Keras API\n    inputLayer = keras.Input(shape=(evaluationsPerImage * 6,))\n    layer1 = keras.layers.Dense(576, activation='relu')(inputLayer)\n    layer2 = keras.layers.Dense(1152, activation='relu')(layer1)\n    layer7 = keras.layers.Dense(576, activation='relu')(layer2)\n    layer8 = keras.layers.Dense(144, activation='relu')(layer7)\n    outputLayer = keras.layers.Dense(numberOfClasses, activation='softmax')(layer8)\n    model = keras.Model(inputs=inputLayer, outputs=outputLayer)\n    model.compile(\n        optimizer=Adam(lr=0.0001),\n        loss='categorical_crossentropy',\n        metrics=['accuracy', 'categorical_crossentropy', 'categorical_accuracy']\n    )\n    return model\n\ndef trainFinalEvaluationModel():\n    global accumulatedData\n    global accumulatedLabels\n    # load the training data\n    #check if the model exists, if not train it\n    if os.path.isfile('finalLossLogEvaluationModel.h5'):\n        return\n    #check if accumulatedData.npy exists, if not load the data\n    if os.path.isfile('accumulatedData.npy'):\n        accumulatedData = np.load('accumulatedData.npy')\n        accumulatedLabels = np.load('accumulatedLabels.npy')\n        #reduce to numberOfSamples\n        accumulatedData = accumulatedData[:numberOfTrainingSamples]\n        accumulatedLabels = accumulatedLabels[:numberOfTrainingSamples]\n    else:\n        loadTrainingData()\n        np.save('accumulatedData.npy', accumulatedData)\n        np.save('accumulatedLabels.npy', accumulatedLabels)\n    #create the model\n    finalEvaluationModel = createFinalEvaluationLossLogModel()\n    # train the final evaluation model\n    finalEvaluationModel.fit(accumulatedData, accumulatedLabels, epochs=16, batch_size=32) # after 16 epochs overfitting starts\n    # save the final evaluation model\n    finalEvaluationModel.save('finalLossLogEvaluationModel.h5')\n    \n\nif not trainingMode:\n    processImagesForEvaluation()\nelse:\n    trainFinalEvaluationModel()\n","metadata":{"execution":{"iopub.status.busy":"2022-10-05T13:27:55.914653Z","iopub.execute_input":"2022-10-05T13:27:55.915886Z","iopub.status.idle":"2022-10-05T13:32:40.557149Z","shell.execute_reply.started":"2022-10-05T13:27:55.915837Z","shell.execute_reply":"2022-10-05T13:32:40.554879Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"fieldCount: 1 of 2 for image 008e5c_0 done\nfieldCount: 2 of 2 for image 008e5c_0 done\n","output_type":"stream"},{"name":"stderr","text":"2022-10-05 13:28:17.326737: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"image 008e5c_0 done!\nfieldCount: 1 of 12 for image 006388_0 done\nfieldCount: 1 of 4 for image 00c058_0 done\nfieldCount: 2 of 4 for image 00c058_0 done\nfieldCount: 3 of 4 for image 00c058_0 done\nfieldCount: 4 of 4 for image 00c058_0 done\nfieldCount: 2 of 12 for image 006388_0 done\nfieldCount: 1 of 8 for image 01adc5_0 done\nfieldCount: 2 of 8 for image 01adc5_0 done\nfieldCount: 3 of 12 for image 006388_0 done\nfieldCount: 3 of 8 for image 01adc5_0 done\nfieldCount: 4 of 8 for image 01adc5_0 done\nfieldCount: 5 of 8 for image 01adc5_0 done\nfieldCount: 4 of 12 for image 006388_0 done\nfieldCount: 6 of 8 for image 01adc5_0 done\nfieldCount: 7 of 8 for image 01adc5_0 done\nfieldCount: 8 of 8 for image 01adc5_0 done\nfieldCount: 5 of 12 for image 006388_0 done\nfieldCount: 6 of 12 for image 006388_0 done\nfieldCount: 7 of 12 for image 006388_0 done\nfieldCount: 8 of 12 for image 006388_0 done\nfieldCount: 9 of 12 for image 006388_0 done\nfieldCount: 10 of 12 for image 006388_0 done\nfieldCount: 11 of 12 for image 006388_0 done\nfieldCount: 12 of 12 for image 006388_0 done\nimage 006388_0 done!\nimage 01adc5_0 done!\nimage 00c058_0 done!\n","output_type":"stream"}]},{"cell_type":"code","source":"'''#test finalLossLogEvaluationModel.h5 on accumulatedDataTest.npy and accumulatedLabelsTest.npy\n#load the test data\naccumulatedDataTest = np.load('accumulatedDataTest.npy')\naccumulatedLabelsTest = np.load('accumulatedLabelsTest.npy')\n#load the model\nfinalEvaluationModel = keras.models.load_model('finalLossLogEvaluationModel.h5')\n#evaluate the model\nfinalEvaluationModel.evaluate(accumulatedDataTest, accumulatedLabelsTest, batch_size=32, verbose=1)\nresult = finalEvaluationModel.predict(accumulatedDataTest[:50])\nfor i in range(50):\n    print('label: ' + str(accumulatedLabelsTest[i]) + ' prediction: ' + str(result[i]))'''\nif not trainingMode:\n    finalEvaluationModel = keras.models.load_model(logLossEvaluationModelPath + 'finalLossLogEvaluationModel.h5')\n    #dictionary to convert imageId to patientId\n    imageIdToPatientId = {}\n    #dictionary to convert patientId to list of score with number of images like [CE score, LAA score, numberOfImages]\n    patientIdToScore = {}\n    CE_INDEX = 0\n    LAA_INDEX = 1\n    NUMBER_OF_IMAGES_INDEX = 2\n     #iterate through the rows of the dataframe and fill the dictionaries\n    for index, row in dataMetaTable.iterrows():\n        imageId = row['image_id']\n        if row['patient_id'] not in patientIdToScore:\n            patientIdToScore[row['patient_id']] = [0.0, 0.0, 0]\n        # check if the dictionary \"evaluations\" is not None for image id\n        if imageId in evaluations:\n            toPredict = evaluations[imageId].reshape(1, evaluationsPerImage * 6)\n            # predict the image\n            result = finalEvaluationModel.predict(toPredict)\n            currentScore = patientIdToScore[row['patient_id']]\n            currentScore[CE_INDEX] += result[0][CE_INDEX]\n            currentScore[LAA_INDEX] += result[0][LAA_INDEX]\n            currentScore[NUMBER_OF_IMAGES_INDEX] += 1\n\n    #iterate through the patientIdToScore dictionary and calculate the average score\n    patientIdGotScore = {}\n    for index, row in dataMetaTable.iterrows():\n        patientId = row['patient_id']\n        currentScore = patientIdToScore[patientId]\n        if patientId not in patientIdGotScore:\n            if currentScore[NUMBER_OF_IMAGES_INDEX] != 0:\n                currentScore[CE_INDEX] /= currentScore[NUMBER_OF_IMAGES_INDEX]\n                currentScore[LAA_INDEX] /= currentScore[NUMBER_OF_IMAGES_INDEX]\n            else:\n                currentScore[CE_INDEX] = 0.7 #if no information, CE is statistical more likely\n                currentScore[LAA_INDEX] = 0.3\n            patientIdGotScore[patientId] = True\n    \n    submission = pd.DataFrame(columns=['patient_id', 'CE', 'LAA'])\n    for patientId in patientIdToScore:\n        submission = submission.append({'patient_id': patientId, 'CE': patientIdToScore[patientId][CE_INDEX], 'LAA': patientIdToScore[patientId][LAA_INDEX]}, ignore_index=True)\n    submission.to_csv('submission.csv', index=False)\n    print('submission created!')","metadata":{"execution":{"iopub.status.busy":"2022-10-05T13:32:40.563831Z","iopub.execute_input":"2022-10-05T13:32:40.564415Z","iopub.status.idle":"2022-10-05T13:32:41.481570Z","shell.execute_reply.started":"2022-10-05T13:32:40.564357Z","shell.execute_reply":"2022-10-05T13:32:41.480393Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"submission created!\n","output_type":"stream"}]}]}